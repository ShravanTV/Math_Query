services:
  ollama:
    build: ./ollama
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODEL=math-ping-assistant
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 15s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]


  backend:
    build: ./backend
    environment:
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=math-ping-assistant
    ports:
      - "8000:8000"
    depends_on:
      - ollama
    volumes:
      - ./backend:/app


  frontend:
    build: ./frontend
    ports:
      - "8501:8501"
    environment:
      - API_URL=http://backend:8000/query
    depends_on:
      - backend
    volumes:
      - ./frontend:/app
  

  demo-backend:
    build: ./demo_backend
    environment:
      - BACKEND_URL=http://backend:8000/query
    depends_on:
      - ollama
      - backend
    volumes:
      - ./demo_backend:/app
 



